{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook, I trained the PNet, which is first part of the MTCNN model. \n",
    "- Besides, I showed detailed comments of the codes and some formulas in this notebook.  \n",
    "- In order to be more concise, I will not show the repetitive parts in other notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# read file\n",
    "import os.path as osp\n",
    "import os\n",
    "# read & show images\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "# random choose\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy.random import uniform\n",
    "from random import shuffle\n",
    "# others\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Pnet txt file\n",
    "- Here is an example of the lines in the 'wider_face_train_bbx_gt.txt' file:\n",
    "        lfw_5590\\Abdul_Rahman_0001.jpg 71 174 82 185 103.250000 114.750000 147.250000 111.750000 123.250000 132.250000 108.750000 160.250000 145.750000 157.750000\n",
    "  The first part is the image path, the following 4 figures show the box position:\n",
    "      x1, x2, y1, y2\n",
    "  and the last 10 figures show the landmark positions:\n",
    "      x1, y1, x2, y2, x3, y3,x4, y4, x5, y5\n",
    "  \n",
    "\n",
    "- In this part, the output is a list of :\n",
    "        [image path, [box],[landmark]]\n",
    "        \n",
    "- The output will be the input of the InplaceDataSet function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def landmark_dataset_txt_parser(txt_path, img_dir,num_data=None):\n",
    "    \"\"\"\n",
    "    :param txt_path:\n",
    "    :param img_dir:\n",
    "    :return: [absolute_img_path,[x1,x2,y1,y2],(x,y)of[left_eye,right_eye,nose,mouse_left, mouse_right]]\n",
    "    \"\"\"\n",
    "    img_count=0\n",
    "    if txt_path is None or img_dir is None:\n",
    "        return []\n",
    "    if osp.exists(txt_path):\n",
    "        # *** img_faces shape :[img_path,[faces_num, 4]]\n",
    "        img_faces = []\n",
    "        with open(txt_path, 'r') as f:\n",
    "            l = []\n",
    "            lines = list(map(lambda line: line.strip().split('\\n'), f))\n",
    "            # lines[[str],[str],[]...]\n",
    "            lines = [i[0].split(' ') for i in lines]\n",
    "            # lines [[path_str,pos_str]...]\n",
    "            for line in lines:\n",
    "                img_path = line[0].replace('\\\\', '/')\n",
    "                faces_pos = [int(i) for i in line[1:5]]\n",
    "                landmark = [float(i) for i in line[5:]]\n",
    "                real_img_path = osp.join(img_dir, img_path)\n",
    "                if osp.exists(real_img_path):\n",
    "                    try:\n",
    "                        Image.open(real_img_path).verify()\n",
    "                        img_faces.append([real_img_path, faces_pos, landmark])\n",
    "                        img_count+=1\n",
    "                    except:\n",
    "                        print('Invalid image',real_img_path)\n",
    "                else:\n",
    "                    print(\"*** warning:image path invalid\")\n",
    "                                           \n",
    "                if num_data is not None and img_count>=num_data:\n",
    "                    break\n",
    "                                           \n",
    "        return img_faces\n",
    "    else:\n",
    "        print('*** warning:WILDER_FACE txt file not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Inplace Dataset\n",
    "- The input of this part is generated from the **create_pnet_data_txt_parser** function\n",
    "\n",
    "- The output of the **self.\\__getitem \\__ ** will be:\n",
    "      (img_tensor, label, offset, landmark_flag, landmark)\n",
    "      \n",
    "      img_tensor: the tensor of the cropped image  \n",
    "      label: 1 if have face, else 0  \n",
    "      offset: calculated by the box offset formula  \n",
    "      landmark_flag: 1 if have landmark, else 0  \n",
    "      landmark: calculated by the landmark offset formula\n",
    "      \n",
    "- The **self.\\__getitem \\__ ** do the following things:  \n",
    "\n",
    "    * It opens the corresponding image, and get the figures of face box (x1, y1, w, h)\n",
    "    * It randomly cropped a box in the image, with a ratio:  \n",
    "            negative (no face) : positive (have face) :  part face  =  3 : 1 : 1 \n",
    "    * It turn the label into figures:\n",
    "            negative --> 0, positive --> 1, part face --> 1\n",
    "    * It calulated the offset of the cropped box\n",
    "    \n",
    " \n",
    "- The box offset formula:\n",
    "        ground truth: [ x1,  y1,  x2,  y2 ]\n",
    "        cropped box:  [ bx1, by1, bx2, by2] \n",
    "$$ a_1=\\frac{x1-bx1}{bx2-bx1}$$\n",
    "\n",
    "$$ a_2=\\frac{y1-by1}{by2-by1}$$\n",
    "\n",
    "$$ a_3=\\frac{x2-bx2}{bx2-bx1}$$\n",
    "\n",
    "$$ a_4=\\frac{y2-by2}{by2-by1}$$\n",
    "\n",
    "$$ offset =  [ a_1, a_2, a_3, a_4]$$\n",
    "\n",
    "- The landmark offset formula:\n",
    "        landmark:    [x1,  y1,  x2,  y2 ,x3,  y3,  x4,  y4 , x5,  y5]\n",
    "        cropped box:  [ bx1, by1, bx2, by2]\n",
    "$$ Lx_i=\\frac{xi-bx1}{bx2-bx1} $$\n",
    "$$ Ly_i=\\frac{yi-by1}{by2-by1} $$\n",
    "\n",
    "$$ offset =  [ Lx_i,\\ \\ Ly_i\\ \\ for\\ i\\ in\\ range(5)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InplaceDataset(data.Dataset):\n",
    "    def __init__(self,  img_face_landmark, cropsize, img_faces=[],pnet=None, rnet=None, ratio=(2, 1, 1, 1)):\n",
    "        \"\"\"\n",
    "        :param train_data_list: [train_data_num,[img_path,labels,[offsets],[landmark]]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # self.img_faces = img_faces + img_face_landmark\n",
    "        self.img_faces = img_face_landmark + img_faces\n",
    "        shuffle(self.img_faces)\n",
    "        self.crop_size = cropsize\n",
    "        self.pnet = pnet\n",
    "        self.rnet = rnet\n",
    "        ratio_sum = float(sum(ratio))\n",
    "        self.ratio = [i / ratio_sum for i in ratio]\n",
    "        self.cache = []\n",
    "        print('===> data set size:{}'.format(self.__len__()))\n",
    "        # self.ratio = ratio\n",
    "        # self.dict = {'p': 0.0, 'pf': 0.0, 'l': 1.0, 'n': 0.0}\n",
    "\n",
    "    def get_img_faces_ldmk(self, index):\n",
    "        def load_img(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')\n",
    "            except Exception:\n",
    "                print('*** warning loading fail!')\n",
    "                return\n",
    "            return img\n",
    "\n",
    "        img_face = self.img_faces[index]\n",
    "        img_path = img_face[0]\n",
    "        faces = np.array(img_face[1])\n",
    "        \n",
    "        \n",
    "      \n",
    "        if faces.ndim is 1:\n",
    "            # img_face_landmark\n",
    "            # [absolute_img_path,[x1,x2,y1,y2],(x,y)of[left_eye,right_eye,nose,mouse_left, mouse_right]]\n",
    "            try:\n",
    "                faces = np.expand_dims(faces, 0)\n",
    "                faces[:, :] = faces[:, (0, 2, 1, 3)]\n",
    "            except:\n",
    "                print('error:',img_path)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # [img_num * [absolute_img_path, [faces_num * 4(which is x1, y1, w, h)]]]\n",
    "            faces[:, 2] += faces[:, 0]\n",
    "            faces[:, 3] += faces[:, 1]\n",
    "        ldmk = None if len(img_face) < 3 else [int(i) for i in img_face[2]]\n",
    "\n",
    "        return load_img(img_path), faces, ldmk\n",
    "\n",
    "    def get_crop_img_label_offset_ldmk(self, img, faces, ldmk, index):\n",
    "        def get_crop_img(img_np, crop_box, crop_size):\n",
    "            crop_box = [int(i) for i in crop_box]\n",
    "            crop_img_np = img_np[crop_box[1]:crop_box[3], crop_box[0]:crop_box[2], :]\n",
    "            crop_img = Image.fromarray(crop_img_np, mode='RGB')\n",
    "            crop_img = crop_img.resize((crop_size, crop_size), resample=PIL.Image.BILINEAR)\n",
    "            return crop_img\n",
    "\n",
    "        def get_real_label(label):\n",
    "            return {'n': 'n', 'np': 'n', 'pf': 'pf' if ldmk is None else 'l',\n",
    "                    'p': 'p' if ldmk is None else 'l'}.get(label)\n",
    "\n",
    "        def cal_offset(face, box):\n",
    "            if box is None:\n",
    "                return []\n",
    "            offset = [\n",
    "                (face[0] - box[0]) / float(box[2] - box[0]),\n",
    "                (face[1] - box[1]) / float(box[3] - box[1]),\n",
    "                (face[2] - box[2]) / float(box[2] - box[0]),\n",
    "                (face[3] - box[3]) / float(box[3] - box[1]),\n",
    "            ]\n",
    "            return offset\n",
    "\n",
    "        def cal_landmark_offset(box, ldmk):\n",
    "            if ldmk is None or box is None:\n",
    "                return []\n",
    "            else:\n",
    "                minx, miny = box[0], box[1]\n",
    "                w, h = box[2] - box[0], box[3] - box[1]\n",
    "                ldmk_offset = [(ldmk[i] - [minx, miny][i % 2]) / float([w, h][i % 2]) for i in range(len(ldmk))]\n",
    "                return ldmk_offset\n",
    "\n",
    "        img_np = np.array(img)\n",
    "        width, height = img.size\n",
    "        \n",
    "        if self.pnet is None:\n",
    "            # negative, negative partial, partial face, positive\n",
    "            label = random.choice(['n', 'np', 'pf', 'p'], p=self.ratio)\n",
    "\n",
    "            iou_th = {'n': (0, 0.3), 'np': (0, 0.3), 'pf': (0.4, 0.65), 'p': (0.65, 1.0)}.get(label)\n",
    "            sigma = {'n': 1, 'np': 0.3, 'pf': 0.1, 'p': 0.02}.get(label)\n",
    "            face, face_max_size = None, None\n",
    "            for i in range(10):\n",
    "                face = faces[random.randint(len(faces))]\n",
    "                face_max_size = max(face[2] - face[0], face[3] - face[1])\n",
    "                if face_max_size > self.crop_size:\n",
    "                    break\n",
    "            crop_img = None\n",
    "            crop_box = None\n",
    "            for i in range(10):\n",
    "                # if ct >= sample_num: break\n",
    "                max_size = min(width, height)\n",
    "                size = (uniform(-1.0, 1.0) * sigma + 1) * face_max_size\n",
    "                size = min(max(self.crop_size, size), max_size)\n",
    "                x1, y1 = face[0], face[1]\n",
    "                crop_x1, crop_y1 = (uniform(-1.0, 1.0) * sigma + 1) * x1, (uniform(-1.0, 1.0) * sigma + 1) * y1\n",
    "                crop_x1, crop_y1 = min(max(0, crop_x1), width - size), min(max(0, crop_y1), height - size)\n",
    "                crop_box = np.array([int(crop_x1), int(crop_y1), int(crop_x1 + size), int(crop_y1 + size)])\n",
    "                iou = IoU(crop_box, np.array([face]))\n",
    "                iou_max_idx = iou.argmax()\n",
    "                iou = iou.max()\n",
    "                if iou < iou_th[0] or iou > iou_th[1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    crop_img = get_crop_img(img_np, crop_box, self.crop_size)\n",
    "                    break\n",
    "            return crop_img, get_real_label(label), cal_offset(face, crop_box), cal_landmark_offset(crop_box, ldmk)\n",
    "       \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, faces, ldmk = self.get_img_faces_ldmk(index)\n",
    "        crop_img, label, offset, ldmk = self.get_crop_img_label_offset_ldmk(img, faces, ldmk, index)\n",
    "        if crop_img is None: return self.__getitem__(random.randint(0, self.__len__()))\n",
    "        img_tensor = transforms.ToTensor()(crop_img)\n",
    "        landmark_flag = torch.FloatTensor([1.0 if label == 'l' else 0.0])\n",
    "        label = torch.FloatTensor([1.0 if label in ['p', 'pf', 'l'] else 0.0])  \n",
    "        offset = torch.FloatTensor(offset if 4 == len(offset) else 4 * [0.0])\n",
    "        \n",
    "        if 10 != len(ldmk):\n",
    "            print(index,\"NO ldmk\")        \n",
    "        \n",
    "        landmark = torch.FloatTensor(ldmk if 10 == len(ldmk) else 10 * [0.0])\n",
    "        return (img_tensor, label, offset, landmark_flag, landmark)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "\n",
    "class P_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            # 12x12x3\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # PReLU1\n",
    "            # 10x10x10\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool1\n",
    "            # 5x5x10\n",
    "            nn.Conv2d(10, 16, kernel_size=3, stride=1),  # conv2\n",
    "            # 3x3x16\n",
    "            nn.PReLU(),  # PReLU2\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),  # conv3\n",
    "            # 1x1x32\n",
    "            nn.PReLU()  # PReLU3\n",
    "        )\n",
    "        # detection\n",
    "        self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1)\n",
    "        # bounding box regresion\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1)\n",
    "        # landmark localization\n",
    "        self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1)\n",
    "        # weight initiation with xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        det = torch.sigmoid(self.conv4_1(x))\n",
    "        box = self.conv4_2(x)\n",
    "        landmark = self.conv4_3(x)\n",
    "        # det:[,2,1,1], box:[,4,1,1], landmark:[,10,1,1]\n",
    "        return det, box, landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and IoU Accuracy\n",
    "\n",
    "##### Loss Function:  \n",
    "- Face Classification: ( cls_loss)\n",
    "$$ L^{det}_i = -\\ ( y_i^{det}\\ log (p_i)\\ +\\ (1 -  y_i^{det})\\ (1\\ -\\ log ( p_i ) ) ) $$\n",
    "\n",
    "- Bounding Box Regression: ( box_loss)\n",
    "\n",
    "$$ L^{box}_i = ||\\ \\hat{y}^{\\ box}_i\\ -\\ y_i^{\\ box}\\ ||_2^2$$  \n",
    " \n",
    "- Facial landmark Regression: ( box_loss)\n",
    "\n",
    "$$ L^{landmark}_i = ||\\ \\hat{y}^{\\ landmark}_i\\ -\\ y_i^{\\ landmark}\\ ||_2^2$$\n",
    "\n",
    "##### IoU: \n",
    "$$ \\frac{inter\\ area}{total\\ area\\ -\\ inter\\ area}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossFn:\n",
    "    def __init__(self, cls_factor=1, box_factor=1, landmark_factor=1):\n",
    "        # loss function\n",
    "        self.cls_factor = cls_factor\n",
    "        self.box_factor = box_factor\n",
    "        self.land_factor = landmark_factor\n",
    "        self.loss_cls = nn.BCELoss()\n",
    "        self.loss_box = nn.MSELoss()\n",
    "        self.loss_landmark = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "    def cls_loss(self, gt_label, pred_label):\n",
    "        # pred_label: [batch_size, 1, 1, 1] to [batch_size]\n",
    "        pred_label = torch.squeeze(pred_label)\n",
    "        # gt_label: [batch_size, 1] to [batch_size ]\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "        # get the mask element which >= 0, only 0 and 1 can effect the detection loss\n",
    "        # mask = torch.ge(gt_label, 0)\n",
    "        mask = torch.ge(gt_label, 0)        \n",
    "        valid_gt_label = gt_label[mask]\n",
    "        valid_pred_label = pred_label[mask]\n",
    "        \n",
    "        return self.loss_cls(valid_pred_label, valid_gt_label) * self.cls_factor\n",
    "    \n",
    "    \n",
    "\n",
    "    def box_loss(self, gt_label, gt_offset, pred_offset):\n",
    "        # if gt_label is torch.tensor([0.0]):\n",
    "        #     return torch.tensor([0.0])\n",
    "        # pred_offset: [batch_size, 4] to [batch_size,4]\n",
    "        pred_offset = torch.squeeze(pred_offset)\n",
    "        # gt_offset: [batch_size, 4, 1, 1] to [batch_size,4]\n",
    "        gt_offset = torch.squeeze(gt_offset)\n",
    "        # gt_label: [batch_size, 1, 1, 1] to [batch_size]\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "\n",
    "        # get the mask element which != 0\n",
    "        # unmask = torch.eq(gt_label, 0)\n",
    "        # mask = torch.eq(unmask, 0)\n",
    "        mask = torch.eq(gt_label, 1)\n",
    "        # convert mask to dim index\n",
    "        \n",
    "        valid_gt_offset = gt_offset[mask, :]\n",
    "        valid_pred_offset = pred_offset[mask, :]\n",
    "        valid_sample_num = valid_gt_offset.shape[0]\n",
    "        if 0 == valid_sample_num:\n",
    "            return torch.tensor([0.0])\n",
    "        else:\n",
    "            return self.loss_box(valid_pred_offset, valid_gt_offset) * self.box_factor\n",
    "\n",
    "    def landmark_loss(self, landmark_flag, gt_landmark=None, pred_landmark=None):\n",
    "        # pred_landmark:[batch_size,10,1,1] to [batch_size,10]\n",
    "        pred_landmark = torch.squeeze(pred_landmark)\n",
    "        # gt_landmark:[batch_size,10] to [batch_size,10]\n",
    "        gt_landmark = torch.squeeze(gt_landmark)\n",
    "        # gt_label:[batch_size,1] to [batch_size]\n",
    "        gt_label = torch.squeeze(landmark_flag)\n",
    "        mask = torch.eq(gt_label, 1)\n",
    "        valid_gt_landmark = gt_landmark[mask, :]\n",
    "        valid_pred_landmark = pred_landmark[mask, :]\n",
    "        valid_sample_num = valid_gt_landmark.shape[0]\n",
    "        if 0 == valid_sample_num:\n",
    "            return torch.tensor([0.0])\n",
    "        else:\n",
    "            return self.loss_landmark(valid_pred_landmark, valid_gt_landmark) * self.land_factor\n",
    "\n",
    "    def total_loss(self, gt_label, pred_label, gt_offset, pred_offset, landmark_flag, gt_landmark, pred_landmark):\n",
    "        return self.cls_loss(gt_label, pred_label) \\\n",
    "               + self.box_loss(gt_label, gt_offset, pred_offset) \\\n",
    "               + self.landmark_loss(landmark_flag, gt_landmark, pred_landmark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IoU(box, boxes):\n",
    "    \"\"\"\n",
    "    Compute IoU between detect box and gt boxes\n",
    "    \"\"\"\n",
    "    # box = (x1, y1, x2, y2)\n",
    "    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "\n",
    "    # abtain the offset of the interception of union between crop_box and gt_box\n",
    "    xx1 = np.maximum(box[0], boxes[:, 0])\n",
    "    yy1 = np.maximum(box[1], boxes[:, 1])\n",
    "    xx2 = np.minimum(box[2], boxes[:, 2])\n",
    "    yy2 = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    # compute the width and height of the bounding box\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    ovr = inter / (box_area + area - inter)\n",
    "    return ovr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(net, net_name='pnet',  loss_config=[],patient=5):\n",
    "    train_losses=[]\n",
    "    eval_losses=[]\n",
    "    best_eval_loss=100\n",
    "    best_eval_weight=copy.deepcopy(net.state_dict())\n",
    "    stop_count=0\n",
    "            \n",
    "    optimizer = opt.Adam(net.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, amsgrad=True)\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, \n",
    "        verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=2, min_lr=1e-08, eps=1e-08)\n",
    "    loss = LossFn(cls_factor=loss_config[0], box_factor=loss_config[1], landmark_factor=loss_config[2])\n",
    "    \n",
    "    iter_count=0\n",
    "    for epoch in range(EPOCH):\n",
    "        net.train()\n",
    "        t0 = time.perf_counter()\n",
    "        train_loss=0        \n",
    "        for (img_tensor, label, offset, landmark_flag, landmark) in Train_Loader:\n",
    "            iter_count += 1\n",
    "            wrap = (img_tensor, label, offset, landmark)\n",
    "            (img_tensor, label, offset, landmark) = [i.cuda() for i in wrap]\n",
    "            \n",
    "            \n",
    "            \n",
    "            det, box, ldmk = net(img_tensor)\n",
    "            if landmark==None:\n",
    "                ldmk=None\n",
    "                print('no ldmk')\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            all_loss = loss.total_loss(gt_label=label, pred_label=det, gt_offset=offset, pred_offset=box,\n",
    "                                       landmark_flag=landmark_flag, pred_landmark=ldmk, gt_landmark=landmark)\n",
    "            all_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=all_loss.item()* img_tensor.size(0)\n",
    "            \n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        train_loss=train_loss/train_num\n",
    "        train_losses.append(train_loss)\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        print('===> epoch:{}\\t| train_loss:{:.8f}\\t| time:{:.8f}s'.format(epoch, train_loss, t1 - t0)) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_loss=0\n",
    "            net.eval()\n",
    "            for (img_tensor, label, offset, landmark_flag, landmark) in Eval_Loader:\n",
    "                wrap = (img_tensor, label, offset, landmark)\n",
    "                (img_tensor, label, offset, landmark) = [i.cuda() for i in wrap]\n",
    "                det, box, ldmk = net(img_tensor)\n",
    "                if landmark==None:\n",
    "                    ldmk=None            \n",
    "                all_loss = loss.total_loss(gt_label=label, pred_label=det, gt_offset=offset, pred_offset=box,\n",
    "                                           landmark_flag=landmark_flag, pred_landmark=ldmk, gt_landmark=landmark)\n",
    "                eval_loss+=all_loss.item() * img_tensor.size(0)  \n",
    "                \n",
    "                \n",
    "        t2 = time.perf_counter()\n",
    "        eval_loss=eval_loss/eval_num\n",
    "        eval_losses.append(eval_loss)\n",
    "        print('                | eval_loss:{:.8f}\\t| time:{:.8f}s'.format(eval_loss, t2 - t1))\n",
    "        if eval_loss< best_eval_loss:\n",
    "            best_eval_loss=eval_loss\n",
    "            best_eval_weight=copy.deepcopy(net.state_dict())\n",
    "            stop_count=0\n",
    "        else:\n",
    "            stop_count+=1\n",
    "        \n",
    "        if stop_count >= patient and patient is not None:\n",
    "            break\n",
    "            \n",
    "    torch.save(best_eval_weight, f'/kaggle/working/{train_num}_{EPOCH} best.pkl')\n",
    "    torch.save(net.state_dict(), f'/kaggle/working/{train_num}_{EPOCH}.pkl')\n",
    "    print('Best Eval Loss:',best_eval_loss)\n",
    "    return train_losses, eval_losses,net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> data set size:10000\n",
      "===> data set size:3466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0')\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "# train_txt_path='../input/wider-data/WIDER/wider_face_train_bbx_gt.txt'\n",
    "# eval_txt_path='../input/wider-data/WIDER/wider_face_val_bbx_gt.txt'\n",
    "# train_img_path='../input/wider-data/WIDER/WIDER_train'\n",
    "# eval_img_path='../input/wider-data/WIDER/WIDER_val'\n",
    "train_num=None\n",
    "eval_num=None\n",
    "class_data_augment = 3 \n",
    "landmark_data_dir = '../input/facial-point-detection/Facial_Point_Detection'\n",
    "landmark_train_txt_path = '../input/facial-point-detection/Facial_Point_Detection/trainImageList.txt'\n",
    "landmark_eval_txt_path = '../input/facial-point-detection/Facial_Point_Detection/testImageList.txt'\n",
    "LR = 0.001\n",
    "EPOCH = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKS = 8\n",
    "Pnet_loss_config = [1.0, 0.5, 0.5]\n",
    "pnet_weight_path = '../input/for-rnet/pnet_10000_40.pkl'\n",
    "\n",
    "# train_img_faces = create_pnet_data_txt_parser(train_txt_path,train_img_path ,num_data=train_num)\n",
    "# eval_img_faces = create_pnet_data_txt_parser(eval_txt_path,eval_img_path ,num_data=eval_num)\n",
    "landmark_train=landmark_dataset_txt_parser(landmark_train_txt_path, landmark_data_dir,num_data=train_num)\n",
    "landmark_eval=landmark_dataset_txt_parser(landmark_eval_txt_path, landmark_data_dir,num_data=eval_num)\n",
    "train_IDS=InplaceDataset(landmark_train, cropsize=12)\n",
    "eval_IDS=InplaceDataset(landmark_eval, cropsize=12)\n",
    "Train_Loader=DataLoader(train_IDS,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKS,pin_memory=False)\n",
    "Eval_Loader=DataLoader(eval_IDS,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKS,pin_memory=False)\n",
    "train_num=len(train_IDS)\n",
    "eval_num=len(eval_IDS)\n",
    "\n",
    "net=P_Net().cuda()\n",
    "net.load_state_dict(torch.load(pnet_weight_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> epoch:0\t| train_loss:0.15564792\t| time:54.75159675s\n",
      "                | eval_loss:0.07900422\t| time:18.87199519s\n",
      "===> epoch:1\t| train_loss:0.08456848\t| time:54.61264215s\n",
      "                | eval_loss:0.08624503\t| time:18.94211011s\n",
      "===> epoch:2\t| train_loss:0.06828545\t| time:54.23650294s\n",
      "                | eval_loss:0.08843866\t| time:18.82516556s\n",
      "===> epoch:3\t| train_loss:0.06651744\t| time:54.43137760s\n",
      "                | eval_loss:0.05909705\t| time:19.95284436s\n",
      "===> epoch:4\t| train_loss:0.05218322\t| time:54.29483828s\n",
      "                | eval_loss:0.06759226\t| time:19.27663746s\n",
      "===> epoch:5\t| train_loss:0.05169713\t| time:54.87617830s\n",
      "                | eval_loss:0.04650763\t| time:19.22434270s\n",
      "===> epoch:6\t| train_loss:0.04919850\t| time:55.15196533s\n",
      "                | eval_loss:0.04323545\t| time:19.45507934s\n",
      "===> epoch:7\t| train_loss:0.04494784\t| time:53.34835946s\n",
      "                | eval_loss:0.04442728\t| time:20.04367854s\n",
      "===> epoch:8\t| train_loss:0.04055698\t| time:54.53434050s\n",
      "                | eval_loss:0.03727839\t| time:19.62053354s\n",
      "===> epoch:9\t| train_loss:0.04381116\t| time:54.92655002s\n",
      "                | eval_loss:0.04846932\t| time:19.09213008s\n",
      "===> epoch:10\t| train_loss:0.03592768\t| time:53.87952609s\n",
      "                | eval_loss:0.04561732\t| time:19.01327121s\n",
      "===> epoch:11\t| train_loss:0.03998440\t| time:54.39650120s\n",
      "                | eval_loss:0.03727223\t| time:19.34535490s\n",
      "===> epoch:12\t| train_loss:0.03530664\t| time:54.35352178s\n",
      "                | eval_loss:0.03131011\t| time:20.29493033s\n",
      "===> epoch:13\t| train_loss:0.03434740\t| time:54.03989089s\n",
      "                | eval_loss:0.04306335\t| time:18.78145213s\n",
      "===> epoch:14\t| train_loss:0.03544625\t| time:54.66154747s\n",
      "                | eval_loss:0.02965698\t| time:18.79918713s\n",
      "===> epoch:15\t| train_loss:0.02953184\t| time:54.88719179s\n",
      "                | eval_loss:0.04405605\t| time:19.24657645s\n",
      "===> epoch:16\t| train_loss:0.02863794\t| time:54.04143512s\n",
      "                | eval_loss:0.03821960\t| time:20.15472119s\n",
      "===> epoch:17\t| train_loss:0.03011519\t| time:55.15887620s\n",
      "                | eval_loss:0.03575565\t| time:19.20786281s\n",
      "===> epoch:18\t| train_loss:0.02926357\t| time:54.87894412s\n",
      "                | eval_loss:0.04373361\t| time:19.14984062s\n",
      "===> epoch:19\t| train_loss:0.02579690\t| time:54.96402789s\n",
      "                | eval_loss:0.02871713\t| time:19.17488382s\n",
      "===> epoch:20\t| train_loss:0.02935946\t| time:54.03379055s\n",
      "                | eval_loss:0.02505890\t| time:19.94068805s\n",
      "===> epoch:21\t| train_loss:0.02684959\t| time:53.42604268s\n",
      "                | eval_loss:0.04941858\t| time:19.80280467s\n",
      "===> epoch:22\t| train_loss:0.02587661\t| time:54.59468658s\n",
      "                | eval_loss:0.03290307\t| time:19.16054101s\n",
      "===> epoch:23\t| train_loss:0.02996284\t| time:54.65259345s\n",
      "                | eval_loss:0.03040413\t| time:18.67078923s\n",
      "===> epoch:24\t| train_loss:0.02813999\t| time:54.86889762s\n",
      "                | eval_loss:0.03384691\t| time:20.10319235s\n",
      "===> epoch:25\t| train_loss:0.02326303\t| time:53.73548326s\n",
      "                | eval_loss:0.03737989\t| time:20.47925172s\n",
      "Best Eval Loss: 0.025058897376324888\n"
     ]
    }
   ],
   "source": [
    "train_losses,eval_losses,pnet=train_net(net, net_name='pnet',  loss_config=Pnet_loss_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.arange(len(train_losses))\n",
    "plt.plot(x,train_losses,label='train')\n",
    "plt.plot(x,eval_losses,label='eval')\n",
    "plt.title('Landmark PNet Train')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
