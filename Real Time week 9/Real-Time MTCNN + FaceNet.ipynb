{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy.random import uniform\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "import cv2\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PNet, RNet, ONet boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_square(bboxes):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes to a square form.\n",
    "    \"\"\"\n",
    "    square_bboxes = np.zeros_like(bboxes)\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    h = y2 - y1 + 1.0\n",
    "    w = x2 - x1 + 1.0\n",
    "    max_side = np.maximum(h, w)\n",
    "    square_bboxes[:, 0] = x1 + w * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 1] = y1 + h * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n",
    "    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n",
    "    return square_bboxes\n",
    "\n",
    "def calibrate_box(bboxes, offsets):\n",
    "    \"\"\"\n",
    "        Transform bounding boxes to be more like true bounding boxes.\n",
    "        'offsets' is one of the outputs of the nets.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    w = x2 - x1 + 1.0\n",
    "    h = y2 - y1 + 1.0\n",
    "    # w [w_len, 1]\n",
    "    w = np.expand_dims(w, 1)\n",
    "    # h [h_len, 1]\n",
    "    h = np.expand_dims(h, 1)\n",
    "\n",
    "    translation = np.hstack([w, h, w, h]) * offsets\n",
    "    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\n",
    "    return bboxes\n",
    "\n",
    "def get_image_boxes(bounding_boxes, img, size=24):\n",
    "    \"\"\" Cut out boxes from the image. \"\"\"\n",
    "    num_boxes = len(bounding_boxes)\n",
    "    width, height = img.size\n",
    "\n",
    "    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\n",
    "    img_boxes = np.zeros((num_boxes, 3, size, size), 'float32')\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        img_box = np.zeros((h[i], w[i], 3), 'uint8')\n",
    "\n",
    "        img_array = np.asarray(img, 'uint8')\n",
    "        # print('img_array.shape:', img_array.shape)\n",
    "        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] = \\\n",
    "            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\n",
    "\n",
    "        img_box = Image.fromarray(img_box)\n",
    "        img_box = img_box.resize((size, size), Image.BILINEAR)\n",
    "        img_box = np.asarray(img_box, 'float32')\n",
    "\n",
    "        img_boxes[i, :, :, :] = img_normalization(img_box)\n",
    "\n",
    "    return img_boxes\n",
    "\n",
    "def correct_bboxes(bboxes, width, height):\n",
    "    \"\"\"\n",
    "        Crop boxes that are too big and get coordinates\n",
    "    with respect to cutouts.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    w, h = x2 - x1 + 1.0, y2 - y1 + 1.0\n",
    "    num_boxes = bboxes.shape[0]\n",
    "\n",
    "    x, y, ex, ey = x1, y1, x2, y2\n",
    "    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\n",
    "    edx, edy = w.copy() - 1.0, h.copy() - 1.0\n",
    "\n",
    "    ind = np.where(ex > width - 1.0)[0]\n",
    "    edx[ind] = w[ind] + width - 2.0 - ex[ind]\n",
    "    ex[ind] = width - 1.0\n",
    "\n",
    "    ind = np.where(ey > height - 1.0)[0]\n",
    "    edy[ind] = h[ind] + height - 2.0 - ey[ind]\n",
    "    ey[ind] = height - 1.0\n",
    "\n",
    "    ind = np.where(x < 0.0)[0]\n",
    "    dx[ind] = 0.0 - x[ind]\n",
    "    x[ind] = 0.0\n",
    "\n",
    "    ind = np.where(y < 0.0)[0]\n",
    "    dy[ind] = 0.0 - y[ind]\n",
    "    y[ind] = 0.0\n",
    "    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\n",
    "    return_list = [i.astype('int32') for i in return_list]\n",
    "\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def img_normalization(img):\n",
    "    \"\"\"Preprocessing step before feeding the network. \"\"\"\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = (img - 127.5) * 0.0078125\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLDS = [0.8, 0.5, 0.9]\n",
    "NMS_THRESHOLDS = [0.9, 0.9, 0.03]\n",
    "MIN_FACE_SIZE = 20.0\n",
    "\n",
    "def pnet_boxes(img, pnet, min_face_size=MIN_FACE_SIZE, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS):\n",
    "    pnet.eval()\n",
    "    width, height = img.size\n",
    "    min_length = min(height, width)\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707  # sqrt(0.5)\n",
    "    scales = []\n",
    "    m = min_detection_size / min_face_size\n",
    "    min_length *= m\n",
    "    factor_count = 0\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m * factor ** factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    # STAGE 1\n",
    "    bounding_boxes = []\n",
    "    for s in scales:  # run P-Net on different scales\n",
    "        boxes = run_first_stage(img, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "    try:\n",
    "        _ = bounding_boxes[0]\n",
    "    except Exception:\n",
    "        img.show()\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return None\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "    return bounding_boxes\n",
    "\n",
    "def rnet_boxes(img, rnet, bounding_boxes, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS, show_boxes=True):\n",
    "    rnet.eval()\n",
    "    if bounding_boxes is None:\n",
    "        return []\n",
    "    img_boxes = get_image_boxes(bounding_boxes, img, size=24)\n",
    "    if len(img_boxes)==0 or img_boxes is None:\n",
    "        return []\n",
    "    img_boxes = torch.FloatTensor(img_boxes)\n",
    "    img_boxes=img_boxes.cuda()\n",
    "    output = rnet(img_boxes)\n",
    "    probs = output[0].data.cpu().numpy()  # shape [n_boxes, 1]\n",
    "    offsets = output[1].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "\n",
    "    keep = np.where(probs[:, 0] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 0].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "    if show_boxes: show_bboxes(img, bounding_boxes, []).show()\n",
    "    return bounding_boxes\n",
    "\n",
    "def onet_boxes(img, onet, bounding_boxes, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS):\n",
    "    onet.eval()\n",
    "    img_boxes = get_image_boxes(bounding_boxes, img, size=48)\n",
    "    img_boxes = torch.FloatTensor(img_boxes)\n",
    "    img_boxes = img_boxes.cuda()\n",
    "    if img_boxes.size(0)==0:\n",
    "        return []\n",
    "    output = onet(img_boxes)\n",
    "    probs = output[0].data.cpu().numpy()\n",
    "    offsets = output[1].data.cpu().numpy()\n",
    "    ldmk=output[2].data.cpu().numpy()\n",
    "\n",
    "    keep = np.where(probs[:, 0] > thresholds[2])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 0].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "    ldmk = ldmk[keep]\n",
    "    \n",
    "    width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
    "    height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
    "    xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
    "    ldmk[:, 0:10:2] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*ldmk[:,  0:10:2]\n",
    "    ldmk[:, 1:10:2] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*ldmk[:, 1:10:2]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[2])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "    ldmk=ldmk[keep]\n",
    "    return bounding_boxes,ldmk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MTCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "\n",
    "class P_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(            \n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # PReLU1            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),            \n",
    "            nn.Conv2d(10, 16, kernel_size=3, stride=1),             \n",
    "            nn.PReLU(),  \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),  \n",
    "            nn.PReLU()  \n",
    "        )\n",
    "       \n",
    "        self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1)        \n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1)        \n",
    "        self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1)       \n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        det = torch.sigmoid(self.conv4_1(x))\n",
    "        box = self.conv4_2(x)\n",
    "        landmark = self.conv4_3(x)\n",
    "        # det:[,2,1,1], box:[,4,1,1], landmark:[,10,1,1]\n",
    "        return det, box, landmark\n",
    "    \n",
    "class R_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            # 24x24x3\n",
    "            nn.Conv2d(3, 28, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(), \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), \n",
    "            nn.Conv2d(28, 48, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), \n",
    "            nn.Conv2d(48, 64, kernel_size=2, stride=1),\n",
    "            nn.PReLU()  # prelu3\n",
    "        )\n",
    "        # 2x2x64\n",
    "        self.conv4 = nn.Linear(64 * 2 * 2, 128)   # 128\n",
    "        self.prelu4 = nn.PReLU() \n",
    "        self.conv5_1 = nn.Linear(128, 1)\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "        self.conv5_3 = nn.Linear(128, 10)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv4(x)\n",
    "        x = self.prelu4(x)\n",
    "        det = torch.sigmoid(self.conv5_1(x))\n",
    "        box = self.conv5_2(x)\n",
    "        landmark = self.conv5_3(x)\n",
    "        return det, box, landmark\n",
    "    \n",
    "class O_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(O_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv5 = nn.Linear(128 * 2 * 2, 256)\n",
    "        self.prelu5 = nn.PReLU()\n",
    "        self.conv6_1 = nn.Linear(256, 1)\n",
    "        self.conv6_2 = nn.Linear(256, 4)\n",
    "        self.conv6_3 = nn.Linear(256, 10)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv5(x)\n",
    "        x = self.prelu5(x)\n",
    "        det = torch.sigmoid(self.conv6_1(x))\n",
    "        box = self.conv6_2(x)\n",
    "        landmark = self.conv6_3(x)\n",
    "        return det, box, landmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model and generate the boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_first_stage(image, net, scale, threshold):\n",
    "    width, height = image.size\n",
    "    sw, sh = math.ceil(width * scale), math.ceil(height * scale)\n",
    "    img = image.resize((sw, sh), Image.BILINEAR)\n",
    "    img = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    img = img.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    output = net(img)\n",
    "    probs = output[0].data.cpu().numpy()[0, 0, :, :]\n",
    "    offsets = output[1].data.cpu().numpy()\n",
    "    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
    "    return boxes[keep]\n",
    "\n",
    "def _generate_bboxes(probs, offsets, scale, threshold):\n",
    "    \n",
    "    stride = 2\n",
    "    cell_size = 12\n",
    "\n",
    "    inds = np.where(probs > threshold)\n",
    "    if inds[0].size == 0:\n",
    "        return np.array([])\n",
    "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
    "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
    "    score = probs[inds[0], inds[1]]\n",
    "    bounding_boxes = np.vstack([\n",
    "        np.round((stride * inds[1] + 1.0) / scale),\n",
    "        np.round((stride * inds[0] + 1.0) / scale),\n",
    "        np.round((stride * inds[1] + 1.0 + cell_size) / scale),\n",
    "        np.round((stride * inds[0] + 1.0 + cell_size) / scale),\n",
    "        score, offsets\n",
    "    ])\n",
    "    return bounding_boxes.T\n",
    "\n",
    "def nms(boxes, overlap_threshold=0.5, mode='union'):\n",
    "    \"\"\" Pure Python NMS baseline. \"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    scores = boxes[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        if mode is 'min':\n",
    "            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n",
    "        else:\n",
    "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= overlap_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Aligment and get cropped faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgSize = [112,112]\n",
    "coord5point = [[30.2946+8.0000, 51.6963], # 112x112\n",
    "               [65.5318+8.0000, 51.6963],\n",
    "               [48.0252+8.0000, 71.7366],\n",
    "               [33.5493+8.0000, 92.3655],\n",
    "               [62.7299+8.0000, 92.3655]]\n",
    "\n",
    "def transformation_from_points(points1, points2):\n",
    "    points1 = points1.astype(np.float64)\n",
    "    points2 = points2.astype(np.float64)\n",
    "    c1 = np.mean(points1, axis=0)\n",
    "    c2 = np.mean(points2, axis=0)\n",
    "    points1 -= c1\n",
    "    points2 -= c2\n",
    "    s1 = np.std(points1)\n",
    "    s2 = np.std(points2)\n",
    "    points1 /= s1\n",
    "    points2 /= s2\n",
    "    U, S, Vt = np.linalg.svd(points1.T * points2)\n",
    "    R = (U * Vt).T\n",
    "    return np.vstack([np.hstack(((s2 / s1) * R,c2.T - (s2 / s1) * R * c1.T)),np.matrix([0., 0., 1.])])\n",
    " \n",
    "def warp_im(img_im, orgi_landmarks,tar_landmarks):\n",
    "    pts1 = np.float64(np.matrix([[point[0], point[1]] for point in orgi_landmarks]))\n",
    "    pts2 = np.float64(np.matrix([[point[0], point[1]] for point in tar_landmarks]))\n",
    "    M = transformation_from_points(pts1, pts2)\n",
    "    dst = cv2.warpAffine(img_im, M[:2], (img_im.shape[1], img_im.shape[0]))\n",
    "    return dst\n",
    "\n",
    "def face_alignment(img_im,bounding_boxes,points): \n",
    "    height,width=img_im.shape[:2]\n",
    "    # Size Parameter\n",
    "    lower_threshold = 100\n",
    "    upper_threshold = 200\n",
    "    cropped_imgs=[]\n",
    "    \n",
    "    if bounding_boxes.shape[0] <=0:\n",
    "        return cropped_imgs\n",
    "    else:\n",
    "        for i in range(bounding_boxes.shape[0]):  \n",
    "            x1, y1, x2, y2 = int(min(bounding_boxes[i][0], min(points[i][0:10:2]))), \\\n",
    "                                int(min(bounding_boxes[i][1], min(points[i][1:10:2]))), \\\n",
    "                                int(max(bounding_boxes[i][2], max(points[i][0:10:2]))), \\\n",
    "                                int(max(bounding_boxes[i][3], max(points[i][1:10:2])))\n",
    "                    \n",
    "            new_x1 = max(int(1.50 * x1 - 0.50 * x2),0)\n",
    "            new_x2 = min(int(1.50 * x2 - 0.50 * x1),width-1)\n",
    "            new_y1 = max(int(1.50 * y1 - 0.50 * y2),0)\n",
    "            new_y2 = min(int(1.50 * y2 - 0.50 * y1),height-1)             \n",
    "                   \n",
    "            left_eye_x = points[i][0:10:2][0]\n",
    "            right_eye_x = points[i][0:10:2][1]\n",
    "            nose_x = points[i][0:10:2][2]\n",
    "            left_mouth_x = points[i][0:10:2][3]\n",
    "            right_mouth_x = points[i][0:10:2][4]\n",
    "            left_eye_y = points[i][1:10:2][0]\n",
    "            right_eye_y = points[i][1:10:2][1]\n",
    "            nose_y = points[i][1:10:2][2]\n",
    "            left_mouth_y = points[i][1:10:2][3]\n",
    "            right_mouth_y = points[i][1:10:2][4]\n",
    "            \n",
    "            new_left_eye_x = left_eye_x - new_x1\n",
    "            new_right_eye_x = right_eye_x - new_x1\n",
    "            new_nose_x = nose_x - new_x1\n",
    "            new_left_mouth_x = left_mouth_x - new_x1\n",
    "            new_right_mouth_x = right_mouth_x - new_x1\n",
    "            new_left_eye_y = left_eye_y - new_y1\n",
    "            new_right_eye_y = right_eye_y - new_y1\n",
    "            new_nose_y = nose_y - new_y1\n",
    "            new_left_mouth_y = left_mouth_y - new_y1\n",
    "            new_right_mouth_y = right_mouth_y - new_y1\n",
    " \n",
    "            face_landmarks = [[new_left_eye_x,new_left_eye_y], \n",
    "                                [new_right_eye_x,new_right_eye_y],\n",
    "                                [new_nose_x,new_nose_y],\n",
    "                                [new_left_mouth_x,new_left_mouth_y],\n",
    "                                [new_right_mouth_x,new_right_mouth_y]]\n",
    "            face = img_im[new_y1: new_y2, new_x1: new_x2]\n",
    "            dst2 = warp_im(face,face_landmarks,coord5point)\n",
    "            crop_im = dst2[0:imgSize[0],0:imgSize[1]]\n",
    "            cropped_imgs.append(crop_im)\n",
    "    return cropped_imgs\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The FaceNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "class FaceModel(nn.Module):\n",
    "    def __init__(self,embedding_size=128,pretrained=False):\n",
    "        super(FaceModel, self).__init__()\n",
    "        self.model = resnet18(pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.model.fc = nn.Linear(25088, self.embedding_size)\n",
    "        \n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "        output = _output.view(input_size)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.fc(x)\n",
    "        self.features = self.l2_norm(x)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        alpha=10\n",
    "        self.features = self.features*alpha\n",
    "        return self.features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Calculation of L2 distance\n",
    "$$\\begin{Vmatrix} x_1 - x_2 \\end{Vmatrix}^2_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PairwiseDistance(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(PairwiseDistance, self).__init__()\n",
    "        self.norm = p\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        assert x1.size() == x2.size()\n",
    "        eps = 1e-4 / x1.size(1)\n",
    "        diff = torch.abs(x1 - x2)\n",
    "        out = torch.pow(diff, self.norm).sum(dim=1)\n",
    "        return torch.pow(out + eps, 1. / self.norm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Face Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Parameters and Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pnet_weight_path='../input/landmark/ldmk-pnet_10000_30 best.pkl'\n",
    "rnet_weight_path='../input/landmark/ldmk-rnet_9996_60.pkl'\n",
    "onet_weight_path='../input/landmark/ldmk-onet_ 2 augmentation 100.pkl'\n",
    "facenet_weight_path='../input/facenetweight/train_60_aug.pkl'\n",
    "pnet=P_Net().cuda()\n",
    "pnet.load_state_dict(torch.load(pnet_weight_path))\n",
    "pnet.eval()\n",
    "rnet=R_Net().cuda()\n",
    "rnet.load_state_dict(torch.load(rnet_weight_path))\n",
    "rnet.eval()\n",
    "onet=O_Net().cuda()\n",
    "onet.load_state_dict(torch.load(onet_weight_path))\n",
    "onet.eval()\n",
    "facenet = FaceModel(pretrained=False).cuda()\n",
    "facenet.load_state_dict(torch.load(facenet_weight_path))\n",
    "facenet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get MTCNN and FaceNet outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_face_locations(frame):\n",
    "    p_bounding_boxes = pnet_boxes(frame, pnet, min_face_size=MIN_FACE_SIZE, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS)\n",
    "    r_bounding_boxes=rnet_boxes(frame, rnet, p_bounding_boxes, show_boxes=False)\n",
    "    \n",
    "    if r_bounding_boxes is not None and len(r_bounding_boxes) != 0:\n",
    "        bounding_boxes = np.vstack((p_bounding_boxes, r_bounding_boxes))\n",
    "    else:\n",
    "        bounding_boxes=p_bounding_boxes\n",
    "    if bounding_boxes is not None and len(bounding_boxes) != 0:\n",
    "        o_bounding_boxes,ldmk=onet_boxes(frame, onet,  bounding_boxes)\n",
    "    else:\n",
    "        o_bounding_boxes,ldmk=[],[]\n",
    "\n",
    "    return [o_bounding_boxes,ldmk]\n",
    "        \n",
    "def get_face_encodings(frame,location):\n",
    "    encodings=[]\n",
    "    if len(location[0])==0:\n",
    "        return encodings\n",
    "    bounding_boxes,points=location\n",
    "    cropped_imgs=face_alignment(frame,bounding_boxes,points)\n",
    "    for cropped_img in cropped_imgs:\n",
    "        cropped_img = Image.fromarray(cropped_img[:, :, ::-1])\n",
    "        cropped_img=cropped_img.resize((224,224))\n",
    "        cropped_img=transforms.ToTensor()(cropped_img)\n",
    "        cropped_img=transforms.functional.normalize(cropped_img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        cropped_img=cropped_img.cuda()\n",
    "        cropped_img=torch.unsqueeze(cropped_img,dim=0)\n",
    "        encoding=facenet(cropped_img)\n",
    "        encodings.append(encoding)\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the L2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_dist = PairwiseDistance(2)\n",
    "def get_face_distance(known_faces, a_face):\n",
    "    dists=[]\n",
    "    for known_face in known_faces:\n",
    "        dist=l2_dist.forward(known_face, a_face)\n",
    "        dists.append(dist)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the MTCNN outputs of Known faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_known_face(folder,model):\n",
    "    known_face_encodings=[]\n",
    "    known_face_names=[]\n",
    "    img_name_list=os.listdir(folder)\n",
    "    for i in img_name_list:\n",
    "        path=os.path.join(folder,i)\n",
    "        img=Image.open(path).resize((224,224))\n",
    "        img=transforms.ToTensor()(img)\n",
    "        img=transforms.functional.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        img=torch.unsqueeze(img,dim=0)\n",
    "        img=model(img.cuda())\n",
    "        known_face_encodings.append(img)\n",
    "        known_face_names.append(i.split('_')[0])\n",
    "    return known_face_encodings,known_face_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "known_face_folder='../input/morden-train/Green Book/Green Book'\n",
    "known_face_encodings,known_face_names=get_known_face(known_face_folder,facenet)\n",
    "face_threshold=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Face Information from MTCNN and FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture('../input/morden-train/4.mp4')\n",
    "process_this_frame = True\n",
    "face_infos=[]\n",
    "fram_count=0\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    small_frame=frame\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "    rgb_small_frame=Image.fromarray(rgb_small_frame)\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    face_info=None\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = get_face_locations(rgb_small_frame)\n",
    "        face_encodings = get_face_encodings(small_frame, face_locations)\n",
    "        if len(face_encodings) !=0:\n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                name = \"Unknown\"\n",
    "                face_distances = get_face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if face_distances[best_match_index] < face_threshold:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                face_names.append(name)   \n",
    "            face_locations=[i.tolist() for i in face_locations]\n",
    "            face_info={'face_locations':face_locations,'names':face_names}\n",
    "    face_infos.append(face_info)\n",
    "    fram_count+=1\n",
    "    #process_this_frame = not process_this_frame\n",
    "    \n",
    "with open(\"face_infomation.json\",'w',encoding='utf-8') as json_file:\n",
    "    json.dump(face_infos,json_file,ensure_ascii=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
